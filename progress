1. check this out: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html
https://cnvrg.io/pytorch-lstm/
https://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7

encoder: ena lstm pou pernaei olo to input embedding mia fora
decoder: ena lstm pou pernaei ena ena to input token (enan arithmo sto sequence thn fora) kai bgazei to
probability na einai sygkekrimeno element

12/3
 Questions:
 giati bazoume log probability?

 kanei predict synexeia to idio pragma
TODO:
1. na mhn episkeptete to same node sto idio sequence (aka mask) -> i think ok
2. pame na baloume to last hidden state tou encoder, ston decoder ws hiddens state


Experiments:
1. ebala to last hidden state tou encoder ws hidden state tou decoder. fainetai na einai kalutero
2. ebala ena akomh linear layer ston encoder. ta afhnei 4 output features -> xeiroterepse
ebala na afhnei hidden_size*2 output features ->better loss, shit accuracy


12/05:
1. ebala na pairnei ola ta outputs tou encoder (oxi ta hidden states)