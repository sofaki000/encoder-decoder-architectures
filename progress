TODO:
1. na mhn episkeptete to same node sto idio sequence (aka mask) -> i think ok
2. pame na baloume to last hidden state tou encoder, ston decoder ws hiddens state -> ok
3. add batch size


1. check this out: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html
https://cnvrg.io/pytorch-lstm/
https://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7

encoder: ena lstm pou pernaei olo to input embedding mia fora
decoder: ena lstm pou pernaei ena ena to input token (enan arithmo sto sequence thn fora) kai bgazei to
probability na einai sygkekrimeno element

12/3
 Questions:
 giati bazoume log probability?
 kanei predict synexeia to idio pragma

POIO EINAI TO CONTEXT VECTOR? TI XRHSIMOPOIEI O DECODER?
TO TELEUTAIO OUTPUT TOU ENCODER H TO TELEUTAIO HIDDEN STATE TOU ENCODER?
Apanthsh:
genika to context mporeis na to diamorfwseis opws theleis apo ta encoder
outputs kai ta encoder hidden states! o decoder thelei apla ena context apo ton
encoder. auto mporeis na to kaneis oti thes esy.

Experiments:
1. ebala to last hidden state tou encoder ws hidden state tou decoder. fainetai na einai kalutero
2. ebala ena akomh linear layer ston encoder. ta afhnei 4 output features -> xeiroterepse
ebala na afhnei hidden_size*2 output features ->better loss, shit accuracy


12/05:
1. ebala na pairnei ola ta outputs tou encoder (oxi ta hidden states)
12/06
1. ebala thn dataset klash